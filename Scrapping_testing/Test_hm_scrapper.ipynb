{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), '..', 'DB_and_Azure'))\n",
    "import sql_db_functions as SQLf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def create_soup(url, timeout=5):\n",
    "    def fetch_soup():\n",
    "        nonlocal soup, error\n",
    "        try:\n",
    "            ua = UserAgent()\n",
    "            header = {'User-Agent': str(ua.chrome)}\n",
    "\n",
    "            # Send an HTTP request to the URL\n",
    "            response = requests.get(url, headers=header)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            error = e\n",
    "\n",
    "    soup = None\n",
    "    error = None\n",
    "\n",
    "    thread = threading.Thread(target=fetch_soup)\n",
    "    thread.start()\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        raise TimeoutException(\"Timeout while fetching the URL\")\n",
    "    if error:\n",
    "        raise error\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_required_number(line):\n",
    "    \n",
    "    keeping = ['http','http']\n",
    "    # 001: Prod\n",
    "    # 004: back\n",
    "    # 005: 3/4 image\n",
    "    return any(num in line for num in keeping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_links(prod_soup):\n",
    "    \n",
    "    item_list_1 = prod_soup.select('div div div div div ul li button div img')\n",
    "\n",
    "    ##### get image\n",
    "    images = []\n",
    "    i = 0\n",
    "    for item in item_list_1: \n",
    "            \n",
    "        if (i == len(item_list_1)-3) | (i == len(item_list_1)-1):\n",
    "            images = images + [item['src']]\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(prod_soup):\n",
    "    \n",
    "    description_list = prod_soup.select('dd')\n",
    "\n",
    "    #Get description\n",
    "    description = ''\n",
    "    for d in description_list:\n",
    "        description = description + ' ' + d.text\n",
    "\n",
    "    #Get Materials\n",
    "    description_list = prod_soup.find_all(class_ = 'f8f91e f02838' )\n",
    "\n",
    "    description = description + ' Materials: '\n",
    "\n",
    "    for d in description_list[0:2]:\n",
    "        description = description + ' ' + d.text\n",
    "\n",
    "    return description\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_price(prod_soup):\n",
    "\n",
    "    text = prod_soup.select('span.edbe20')[0].text\n",
    "\n",
    "\n",
    "    # Remove any non-numeric characters except for ',' and '.'\n",
    "    cleaned_text = re.sub(r'[^\\d,\\.]', '', text)\n",
    "    \n",
    "    # Replace comma with a period if there's no period already (to handle decimal part)\n",
    "    if ',' in cleaned_text and '.' not in cleaned_text:\n",
    "        cleaned_text = cleaned_text.replace(',', '.')\n",
    "    elif ',' in cleaned_text and '.' in cleaned_text:\n",
    "        # If both ',' and '.' are present, keep only the period as the decimal separator\n",
    "        cleaned_text = cleaned_text.replace(',', '')\n",
    "    \n",
    "    # Convert the string to a float\n",
    "    number = float(cleaned_text)\n",
    "    \n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HM(category_url,n_products):\n",
    "\n",
    "    soup = create_soup(category_url)\n",
    "    item_list = soup.select('li section article div div div div ul li a')\n",
    "\n",
    "\n",
    "    br = 0\n",
    "    pass_i = 0\n",
    "    for item in item_list:\n",
    "        pass_i +=1\n",
    "        if br == n_products:\n",
    "            break\n",
    "        elif pass_i == 5:\n",
    "\n",
    "            retry_attempts = 3\n",
    "            while retry_attempts > 0:\n",
    "                try:\n",
    "                    # Scrape product details\n",
    "                    try:\n",
    "                        prod_soup = create_soup(item['href'])\n",
    "                    except TimeoutException:\n",
    "                        print(f\"Timeout fetching product {br}. Retrying in 5 seconds...\")\n",
    "                        time.sleep(5)\n",
    "                        retry_attempts -= 1\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching product {br}: {e}\")\n",
    "                        break\n",
    "\n",
    "\n",
    "                    prod_images_links = get_image_links(prod_soup=prod_soup)\n",
    "                    prod_description = get_description(prod_soup=prod_soup)\n",
    "                    prod_price = get_price(prod_soup=prod_soup)\n",
    "\n",
    "                \n",
    "                    print(f'Starting product {br}')\n",
    "                    \n",
    "                    time.sleep(3)\n",
    "                    # Links to image, load to blob and return prod_images_names\n",
    "\n",
    "                    time.sleep(1)\n",
    "                    conn, cursor = SQLf.sql_db_functions.connect_sql()\n",
    "\n",
    "                    SQLf.sql_db_functions.insert_description_image_to_db(\n",
    "                        conn=conn,\n",
    "                        cursor=cursor,\n",
    "                        brand='HM',\n",
    "                        descript=prod_description,\n",
    "                        price=prod_price,\n",
    "                        images_links=prod_images_links\n",
    "                    )\n",
    "                    \n",
    "                    pass_i = 0\n",
    "\n",
    "                    conn.close()\n",
    "                    cursor.close()\n",
    "                    time.sleep(10)\n",
    "                    break  # Exit the retry loop if successful\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing product {br}: {e}\")\n",
    "                    break  # Exit the retry loop if an exception occurs\n",
    "\n",
    "            if retry_attempts == 0:\n",
    "                print(f\"Failed to process product {br} after 3 attempts. Exiting function.\")\n",
    "                return False  # Exit the function if failed after 3 attempts  \n",
    "\n",
    "        br += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting product 4\n",
      "inserting\n",
      "Brand_Prod_id generated: 11\n",
      "Data inserted successfully to DB\n",
      "Starting product 9\n",
      "inserting\n",
      "Brand_Prod_id generated: 12\n",
      "Data inserted successfully to DB\n",
      "Starting product 14\n",
      "inserting\n",
      "Brand_Prod_id generated: 13\n",
      "Data inserted successfully to DB\n",
      "Starting product 19\n",
      "inserting\n",
      "Brand_Prod_id generated: 14\n",
      "Data inserted successfully to DB\n",
      "Starting product 24\n",
      "inserting\n",
      "Brand_Prod_id generated: 15\n",
      "Data inserted successfully to DB\n",
      "Starting product 29\n",
      "inserting\n",
      "Brand_Prod_id generated: 16\n",
      "Data inserted successfully to DB\n",
      "Starting product 34\n",
      "inserting\n",
      "Brand_Prod_id generated: 17\n",
      "Data inserted successfully to DB\n",
      "Starting product 39\n",
      "inserting\n",
      "Brand_Prod_id generated: 18\n",
      "Data inserted successfully to DB\n",
      "Starting product 44\n",
      "inserting\n",
      "Brand_Prod_id generated: 19\n",
      "Data inserted successfully to DB\n",
      "Starting product 49\n",
      "inserting\n",
      "Brand_Prod_id generated: 20\n",
      "Data inserted successfully to DB\n",
      "Starting product 54\n",
      "inserting\n",
      "Brand_Prod_id generated: 21\n",
      "Data inserted successfully to DB\n",
      "Starting product 59\n",
      "inserting\n",
      "Brand_Prod_id generated: 22\n",
      "Data inserted successfully to DB\n",
      "Starting product 64\n",
      "inserting\n",
      "Brand_Prod_id generated: 23\n",
      "Data inserted successfully to DB\n",
      "Starting product 69\n",
      "inserting\n",
      "Brand_Prod_id generated: 24\n",
      "Data inserted successfully to DB\n",
      "Starting product 74\n",
      "inserting\n",
      "Brand_Prod_id generated: 25\n",
      "Data inserted successfully to DB\n",
      "Starting product 79\n",
      "inserting\n",
      "Brand_Prod_id generated: 26\n",
      "Data inserted successfully to DB\n",
      "Starting product 84\n",
      "inserting\n",
      "Brand_Prod_id generated: 27\n",
      "Data inserted successfully to DB\n",
      "Starting product 89\n",
      "inserting\n",
      "Brand_Prod_id generated: 28\n",
      "Data inserted successfully to DB\n",
      "Starting product 94\n",
      "inserting\n",
      "Brand_Prod_id generated: 29\n",
      "Data inserted successfully to DB\n",
      "Starting product 99\n",
      "inserting\n",
      "Brand_Prod_id generated: 30\n",
      "Data inserted successfully to DB\n",
      "Starting product 104\n",
      "inserting\n",
      "Brand_Prod_id generated: 31\n",
      "Data inserted successfully to DB\n",
      "Starting product 109\n",
      "inserting\n",
      "Brand_Prod_id generated: 32\n",
      "Data inserted successfully to DB\n",
      "Starting product 114\n",
      "inserting\n",
      "Brand_Prod_id generated: 33\n",
      "Data inserted successfully to DB\n",
      "Starting product 119\n",
      "inserting\n",
      "Brand_Prod_id generated: 34\n",
      "Data inserted successfully to DB\n"
     ]
    }
   ],
   "source": [
    "# Define the URL you want to scrape\n",
    "url = 'https://www2.hm.com/it_it/donna/acquista-per-prodotto/top.html'\n",
    "\n",
    "\n",
    "# Need to have # of assets times 6\n",
    "#\n",
    "HM(url,120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
